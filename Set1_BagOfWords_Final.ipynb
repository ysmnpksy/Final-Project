{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysmnpksy/Final-Project/blob/main/Set1_BagOfWords_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx5xpSCovCbz"
      },
      "source": [
        "<h1>Final Project Prototype: Deep Learning Algorithm to Recognize Spoilers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajxQZ6oZz8Zi"
      },
      "source": [
        "In order to show proof of concept, I have developed a deep learning bag-of-words algorithm as my prototype. Deep learning allows for pattern recognition in words, sentences, and paragraphs, meaning it can be applied to this project in order to classify whether a piece of text includes a spoiler or not. \n",
        "\n",
        "My development process can be examined through this notebook. I have included explanations and justification in text cells in-between the code to show my understand and demonstrate how I will continue the development phase moving forward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Xt0iyFoQf_"
      },
      "source": [
        "# Part 1: Kaggle & Dataset\n",
        "As the first step, I am preparing the notebook by installing Kaggle and then the dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj0Y1xB5OtCx"
      },
      "source": [
        "## 1.1: Installing Kaggle \n",
        "I have installed Kaggle using the following commands. While the output shows \"already satisfied\", thanks to Google Colab, I am leaving the code here for good practice, as it could be needed when running this notebook on a different platform. \n",
        "\n",
        "I have generated an API login using my Kaggle account and uploaded the JSON file, `kaggle.json`, containing the username and key, to this notebook. If running this notebook, and not viewing via the HTML format, the API key, included in the submission folder, needs to be uploaded to the notebook before running the first code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxNDB9y5lX6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11144a91-4119-4890-8b00-9d136ba492e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "# making kaggle directory \n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "# copying api login info into directory \n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# allocating required permissions \n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YADhpal9ogfk"
      },
      "source": [
        "## 1.2: Installing Dataset \n",
        "\n",
        "The [dataset](www.kaggle.com/rmisra/imdb-spoiler-dataset) used for this prototype and any further iterations of this model is the IMDB Spoiler Dataset, downloaded from Kaggle. This dataset was developed using IMDB and contains two different databases: movies and reviews. The reviews dataset, which will be the one used for this model, contains 573,913 records. \n",
        "\n",
        "IMDB allows users to know which reviews contain spoilers by tagging the reviews with \"Warning: Spoilers\". Reviews which include spoilers but have not been indicated to do so by the reviewer will be removed when reported under the \"Spoiler without warning\" option. Due to this, I can be confident that this dataset includes accurate information, making this dataset was the best option for the project; the only downside is that the reviews are only taken from movies, and not a mix of movies and TV shows, which would have been more applicable to this project. \n",
        "\n",
        "In the code cell below, I am downloading the dataset from Kaggle and unzipping it to make the data accessible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM9aa7vKojIh",
        "outputId": "1d017629-c2d1-4eca-bb30-9682061d540a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading imdb-spoiler-dataset.zip to /content\n",
            " 97% 323M/331M [00:10<00:00, 37.3MB/s]\n",
            "100% 331M/331M [00:10<00:00, 32.5MB/s]\n",
            "Archive:  imdb-spoiler-dataset.zip\n",
            "  inflating: IMDB_movie_details.json  \n",
            "  inflating: IMDB_reviews.json       \n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets download rmisra/imdb-spoiler-dataset\n",
        "\n",
        "# unzipping dataset \n",
        "! unzip imdb-spoiler-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYMmTUbyQCI6"
      },
      "source": [
        "### 1.2.1: Reviews Database \n",
        "\n",
        "As mentioned above, the database I will be using from this dataset is the reviews database, which has over five hundred thousand reviews. This database contains information about each review, such as the date it was made, the movie ID of the movie it was about, the review text itself and a boolean field which indicates whether or not it is a spoiler. \n",
        "\n",
        "The code cell below shows information about this database and a sample consisting of the first 5 reviews. As shown, this dataset consists of 573,913 records, 150,924 of them which contain spoilers. This is a significant number of records and should be enough to train this algorithm without overfitting issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "XmBAiFloyLgB",
        "outputId": "7823aa6d-661c-4702-bb26-79de902aea1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of reviews: 573913\n",
            "Total number of reviews that contain spoilers: 150924\n",
            "User reviews shape: (573913, 7)\n",
            "\n",
            "First 5 user reviews:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        review_date   movie_id    user_id  is_spoiler  \\\n",
              "0  10 February 2006  tt0111161  ur1898687        True   \n",
              "1  6 September 2000  tt0111161  ur0842118        True   \n",
              "2     3 August 2001  tt0111161  ur1285640        True   \n",
              "3  1 September 2002  tt0111161  ur1003471        True   \n",
              "4       20 May 2004  tt0111161  ur0226855        True   \n",
              "\n",
              "                                         review_text  rating  \\\n",
              "0  In its Oscar year, Shawshank Redemption (writt...      10   \n",
              "1  The Shawshank Redemption is without a doubt on...      10   \n",
              "2  I believe that this film is the best story eve...       8   \n",
              "3  **Yes, there are SPOILERS here**This film has ...      10   \n",
              "4  At the heart of this extraordinary movie is a ...       8   \n",
              "\n",
              "                                  review_summary  \n",
              "0  A classic piece of unforgettable film-making.  \n",
              "1     Simply amazing. The best film of the 90's.  \n",
              "2               The best story ever told on film  \n",
              "3                     Busy dying or busy living?  \n",
              "4         Great story, wondrously told and acted  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0e0ac9b-9220-4383-89d5-058d4c4aacb5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_date</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>is_spoiler</th>\n",
              "      <th>review_text</th>\n",
              "      <th>rating</th>\n",
              "      <th>review_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10 February 2006</td>\n",
              "      <td>tt0111161</td>\n",
              "      <td>ur1898687</td>\n",
              "      <td>True</td>\n",
              "      <td>In its Oscar year, Shawshank Redemption (writt...</td>\n",
              "      <td>10</td>\n",
              "      <td>A classic piece of unforgettable film-making.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6 September 2000</td>\n",
              "      <td>tt0111161</td>\n",
              "      <td>ur0842118</td>\n",
              "      <td>True</td>\n",
              "      <td>The Shawshank Redemption is without a doubt on...</td>\n",
              "      <td>10</td>\n",
              "      <td>Simply amazing. The best film of the 90's.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3 August 2001</td>\n",
              "      <td>tt0111161</td>\n",
              "      <td>ur1285640</td>\n",
              "      <td>True</td>\n",
              "      <td>I believe that this film is the best story eve...</td>\n",
              "      <td>8</td>\n",
              "      <td>The best story ever told on film</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1 September 2002</td>\n",
              "      <td>tt0111161</td>\n",
              "      <td>ur1003471</td>\n",
              "      <td>True</td>\n",
              "      <td>**Yes, there are SPOILERS here**This film has ...</td>\n",
              "      <td>10</td>\n",
              "      <td>Busy dying or busy living?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20 May 2004</td>\n",
              "      <td>tt0111161</td>\n",
              "      <td>ur0226855</td>\n",
              "      <td>True</td>\n",
              "      <td>At the heart of this extraordinary movie is a ...</td>\n",
              "      <td>8</td>\n",
              "      <td>Great story, wondrously told and acted</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0e0ac9b-9220-4383-89d5-058d4c4aacb5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b0e0ac9b-9220-4383-89d5-058d4c4aacb5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b0e0ac9b-9220-4383-89d5-058d4c4aacb5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# importing pandas to read the JSON files\n",
        "import pandas as pd \n",
        "\n",
        "# information regarding reviews file\n",
        "all_reviews = pd.read_json('../content/IMDB_reviews.json', lines=True)\n",
        "\n",
        "print('Total number of reviews:', all_reviews['review_date'].count())\n",
        "print('Total number of reviews that contain spoilers:', all_reviews['is_spoiler'].sum())\n",
        "print('User reviews shape:', all_reviews.shape)\n",
        "print()\n",
        "\n",
        "print('First 5 user reviews:')\n",
        "all_reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_reviews.iloc[0].review_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "5KtGdSIeCYZf",
        "outputId": "4975b33a-704f-4b54-e5bc-8a5a014b605f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In its Oscar year, Shawshank Redemption (written and directed by Frank Darabont, after the novella Rita Hayworth and the Shawshank Redemption, by Stephen King) was nominated for seven Academy Awards, and walked away with zero. Best Picture went to Forrest Gump, while Shawshank and Pulp Fiction were \"just happy to be nominated.\" Of course hindsight is 20/20, but while history looks back on Gump as a good film, Pulp and Redemption are remembered as some of the all-time best. Pulp, however, was a success from the word \"go,\" making a huge splash at Cannes and making its writer-director an American master after only two films. For Andy Dufresne and Co., success didn\\'t come easy. Fortunately, failure wasn\\'t a life sentence.After opening on 33 screens with take of $727,327, the $25M film fell fast from theatres and finished with a mere $28.3M. The reasons for failure are many. Firstly, the title is a clunker. While iconic to fans today, in 1994, people knew not and cared not what a \\'Shawshank\\' was. On the DVD, Tim Robbins laughs recounting fans congratulating him on \"that \\'Rickshaw\\' movie.\" Marketing-wise, the film\\'s a nightmare, as \\'prison drama\\' is a tough sell to women, and the story of love between two best friends doesn\\'t spell winner to men. Worst of all, the movie is slow as molasses. As Desson Thomson writes for the Washington Post, \"it wanders down subplots at every opportunity and ignores an abundance of narrative exit points before settling on its finale.\" But it is these same weaknesses that make the film so strong.Firstly, its setting. The opening aerial shots of the prison are a total eye-opener. This is an amazing piece of architecture, strong and Gothic in design. Immediately, the prison becomes a character. It casts its shadow over most of the film, its tall stone walls stretching above every shot. It towers over the men it contains, blotting out all memories of the outside world. Only Andy (Robbins) holds onto hope. It\\'s in music, it\\'s in the sandy beaches of Zihuatanejo; \"In here\\'s where you need it most,\" he says. \"You need it so you don\\'t forget. Forget that there are places in the world that aren\\'t made out of stone. That there\\'s a - there\\'s a - there\\'s something inside that\\'s yours, that they can\\'t touch.\" Red (Morgan Freeman) doesn\\'t think much of Andy at first, picking \"that tall glass o\\' milk with the silver spoon up his ass\" as the first new fish to crack. Andy says not a word, and losing his bet, Red resents him for it. But over time, as the two get to know each other, they quickly become the best of friends. This again, is one of the film\\'s major strengths. Many movies are about love, many flicks have a side-kick to the hero, but Shawshank is the only one I can think of that looks honestly at the love between two best friends. It seems odd that Hollywood would skip this relationship time and again, when it\\'s a feeling that weighs so much into everyone\\'s day to day lives. Perhaps it\\'s too sentimental to seem conventional, but Shawshank\\'s core friendship hits all the right notes, and the film is much better for it.It\\'s pacing is deliberate as well. As we spend the film watching the same actors, it is easy to forget that the movie\\'s timeline spans well over 20 years. Such a huge measure of time would pass slowly in reality, and would only be amplified in prison. And it\\'s not as if the film lacks interest in these moments. It still knows where it\\'s going, it merely intends on taking its sweet time getting there. It pays off as well, as the tedium of prison life makes the climax that much more exhilarating. For anyone who sees it, it is a moment never to be forgotten.With themes of faith and hope, there is a definite religious subtext to be found here. Quiet, selfless and carefree, Andy is an obvious Christ figure. Warden Norton (Bob Gunton) is obviously modeled on Richard Nixon, who, in his day, was as close to a personified Satan as they come. But if you aren\\'t looking for subtexts, the movie speaks to anyone in search of hope. It is a compelling drama, and a very moving film, perfectly written, acted and shot. They just don\\'t come much better than this.OVERALL SCORE: 9.8/10 = A+ The Shawshank Redemption served as a message of hope to Hollywood as well. More than any film in memory, it proved there is life after box office. Besting Forrest and Fiction, it ran solely on strong word of mouth and became the hottest rented film of 1995. It currently sits at #2 in the IMDb\\'s Top 250 Films, occasionally swapping spots with The Godfather as the top ranked film of all time -- redemption indeed. If you haven\\'t seen it yet, what the hell are you waiting for? As Andy says, \"It comes down a simple choice, really. Either get busy living, or get busy dying.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ28Q0ocQF_n"
      },
      "source": [
        "### 1.2.2: Movies Database \n",
        "The second database included in this dataset is the movies database. This database contains all the information regarding each movie itself, such as the genre, duration, and release date. Information about this database and a sample of the first 5 movies can be seen below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "Qp1GEa313Zlr",
        "outputId": "cf7a193e-a365-47dd-a5c6-9e11a90a2313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of movies: 1572\n",
            "Movie details shape: (1572, 7)\n",
            "\n",
            "First 5 movie details:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    movie_id                                       plot_summary  duration  \\\n",
              "0  tt0105112  Former CIA analyst, Jack Ryan is in England wi...  1h 57min   \n",
              "1  tt1204975  Billy (Michael Douglas), Paddy (Robert De Niro...  1h 45min   \n",
              "2  tt0243655  The setting is Camp Firewood, the year 1981. I...  1h 37min   \n",
              "3  tt0040897  Fred C. Dobbs and Bob Curtin, both down on the...   2h 6min   \n",
              "4  tt0126886  Tracy Flick is running unopposed for this year...  1h 43min   \n",
              "\n",
              "                         genre  rating release_date  \\\n",
              "0           [Action, Thriller]     6.9   1992-06-05   \n",
              "1                     [Comedy]     6.6   2013-11-01   \n",
              "2            [Comedy, Romance]     6.7   2002-04-11   \n",
              "3  [Adventure, Drama, Western]     8.3   1948-01-24   \n",
              "4     [Comedy, Drama, Romance]     7.3   1999-05-07   \n",
              "\n",
              "                                       plot_synopsis  \n",
              "0  Jack Ryan (Ford) is on a \"working vacation\" in...  \n",
              "1  Four boys around the age of 10 are friends in ...  \n",
              "2                                                     \n",
              "3  Fred Dobbs (Humphrey Bogart) and Bob Curtin (T...  \n",
              "4  Jim McAllister (Matthew Broderick) is a much-a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2bb9dc18-03d6-40e4-8892-efeaca5db966\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie_id</th>\n",
              "      <th>plot_summary</th>\n",
              "      <th>duration</th>\n",
              "      <th>genre</th>\n",
              "      <th>rating</th>\n",
              "      <th>release_date</th>\n",
              "      <th>plot_synopsis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tt0105112</td>\n",
              "      <td>Former CIA analyst, Jack Ryan is in England wi...</td>\n",
              "      <td>1h 57min</td>\n",
              "      <td>[Action, Thriller]</td>\n",
              "      <td>6.9</td>\n",
              "      <td>1992-06-05</td>\n",
              "      <td>Jack Ryan (Ford) is on a \"working vacation\" in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tt1204975</td>\n",
              "      <td>Billy (Michael Douglas), Paddy (Robert De Niro...</td>\n",
              "      <td>1h 45min</td>\n",
              "      <td>[Comedy]</td>\n",
              "      <td>6.6</td>\n",
              "      <td>2013-11-01</td>\n",
              "      <td>Four boys around the age of 10 are friends in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tt0243655</td>\n",
              "      <td>The setting is Camp Firewood, the year 1981. I...</td>\n",
              "      <td>1h 37min</td>\n",
              "      <td>[Comedy, Romance]</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2002-04-11</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tt0040897</td>\n",
              "      <td>Fred C. Dobbs and Bob Curtin, both down on the...</td>\n",
              "      <td>2h 6min</td>\n",
              "      <td>[Adventure, Drama, Western]</td>\n",
              "      <td>8.3</td>\n",
              "      <td>1948-01-24</td>\n",
              "      <td>Fred Dobbs (Humphrey Bogart) and Bob Curtin (T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tt0126886</td>\n",
              "      <td>Tracy Flick is running unopposed for this year...</td>\n",
              "      <td>1h 43min</td>\n",
              "      <td>[Comedy, Drama, Romance]</td>\n",
              "      <td>7.3</td>\n",
              "      <td>1999-05-07</td>\n",
              "      <td>Jim McAllister (Matthew Broderick) is a much-a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bb9dc18-03d6-40e4-8892-efeaca5db966')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2bb9dc18-03d6-40e4-8892-efeaca5db966 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2bb9dc18-03d6-40e4-8892-efeaca5db966');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# information regarding movie details file \n",
        "all_movies = pd.read_json('../content/IMDB_movie_details.json', lines=True)\n",
        "\n",
        "print('Total number of movies:', all_movies['movie_id'].count())\n",
        "print('Movie details shape:', all_movies.shape)\n",
        "print()\n",
        "\n",
        "print('First 5 movie details:')\n",
        "\n",
        "all_movies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHkXwIyWYZGA"
      },
      "source": [
        "# Part 2: Processing the data \n",
        "\n",
        "The data needs to be processed before a model can be built. Here I will be loading the reviews and the spoiler labels into lists tokenizing using one-hot encoding, and splitting the dataset into training, validation and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8puWZhK7aIRT"
      },
      "source": [
        "## 2.1: Loading the data\n",
        "Here the labels are being added to a `labels` list while the review text is being added to a `texts` list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umJTIEgYYrhq"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "labels = [] \n",
        "texts = []\n",
        "\n",
        "with open('IMDB_reviews.json', 'r') as json_file:\n",
        "  for jsonObj in json_file:\n",
        "    data = json.loads(jsonObj)\n",
        "    if data['is_spoiler'] == True:\n",
        "       labels.append(1)\n",
        "    else:\n",
        "       labels.append(0)\n",
        "    texts.append(data['review_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndIUZ9_naWv8"
      },
      "source": [
        "## 2.2: Tokenizing the text\n",
        "Before splitting, I am tokenizing the text data using one-hot encoding. To do this I am using the `Tokenizer` module from Keras.\n",
        "\n",
        "I have cut the reviews of after a maximum of 500 words and am only considering the most frequent 15000 words in the dataset. \n",
        "\n",
        "After tokenizing, I have vectorized the data using the `pad_sequences` module from Keras, which converts the list of tokenized data, integers, into 2D tensors, which can then be fed into the neural network. I have also vectorized the list of labels by converting it to a list of floating point numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRYt3X4ra3Xq",
        "outputId": "4ecbd0c0-29d3-4487-c2fa-8125600bf42d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 379856 unique tokens.\n",
            "Shape of data tensor: (573913, 500)\n",
            "Shape of label tensor: (573913,)\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 500 # cuts reviews off after 500 words \n",
        "max_words = 15000 # considers only the top 15000 words in the dataset \n",
        "\n",
        "# tokenizing texts \n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "# vectorizing texts\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "# vectorizing labels \n",
        "labels = np.asarray(labels).astype('float32')\n",
        "\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMx46PDU4G8R"
      },
      "source": [
        "## 2.3: Splitting Data\n",
        "Initially, I am splitting the dataset into training and testing sets. This way I will be testing the model on unseen data only, ensuring information leaking does not affect the final results. \n",
        "\n",
        "I am first shuffling the data, since the JSON file lists all reviews including spoilers first, then splitting it. I will be training on 200,000 samples and testing using 100.000 for this prototype.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bulMYFmo4vgK"
      },
      "outputs": [],
      "source": [
        "# suffling data\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# setting number of training and testing samples\n",
        "training_samples = 200000\n",
        "testing_samples = 100000\n",
        "\n",
        "# splitting into training and testing sets\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_test = data[training_samples: training_samples + testing_samples]\n",
        "y_test = labels[training_samples: training_samples + testing_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOu-oo5R6tQp",
        "outputId": "82f50ab0-c8c0-4172-9b32-68f12caefb41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (200000, 500)\n",
            "Training labels shape: (200000,)\n",
            "Test data shape: (100000, 500)\n",
            "Test labels shape: (100000,)\n"
          ]
        }
      ],
      "source": [
        "print('Training data shape:',x_train.shape)\n",
        "print('Training labels shape:', y_train.shape)\n",
        "\n",
        "print('Test data shape:',x_test.shape)\n",
        "print('Test labels shape:',y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcqxCCoLPMvy"
      },
      "source": [
        "I am then further splitting the training set into a partial training and validation set. This way the data used to validate will not be the same as that used to train, which could cause the model to overfit. I will be training on 150,000 samples and validating on 50,000. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjVEeOC0Pida"
      },
      "outputs": [],
      "source": [
        "# setting number of training and validation samples\n",
        "partial_training_samples = 150000\n",
        "validation_samples = 50000\n",
        "\n",
        "# splitting into training and validation sets\n",
        "x_partial_train = data[:partial_training_samples]\n",
        "y_partial_train = labels[:partial_training_samples]\n",
        "x_val = data[partial_training_samples: partial_training_samples + validation_samples]\n",
        "y_val = labels[partial_training_samples: partial_training_samples + validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeEBiHabQDaO",
        "outputId": "22fa0178-28db-429c-8605-1fd0f48bd8a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partial training data shape: (150000, 500)\n",
            "Partial training labels shape: (150000,)\n",
            "Validation data shape: (50000, 500)\n",
            "Validation labels shape: (50000,)\n"
          ]
        }
      ],
      "source": [
        "print('Partial training data shape:',x_partial_train.shape)\n",
        "print('Partial training labels shape:', y_partial_train.shape)\n",
        "\n",
        "print('Validation data shape:',x_val.shape)\n",
        "print('Validation labels shape:',y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPSRZVdN9BO9"
      },
      "source": [
        "# Part 3: The Initial Model\n",
        "\n",
        "Here I am developing the initial model, which I will then tune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vyA94skOsqU"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "from sklearn.metrics import f1_score as f1\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "  tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  posp = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "  predp = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "\n",
        "  precision = tp / (posp + K.epsilon())\n",
        "  recall = tp / (predp + K.epsilon())\n",
        "\n",
        "  f1_score = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "  return f1_score "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMPVPfaBdBuZ"
      },
      "source": [
        "## 3.1: Defining Initial Model \n",
        "\n",
        "The initial model I've defined can be seen below. This model consists of 6 layers. \n",
        "\n",
        "The first two layers are used for word embeddings, which is used to map human language into a geometric space. The `Embedding` layer takes integers as input, finds the integer in an internal dictionary, and returns corresponding word vectors, allowing for vectors to be associated with words. \n",
        "\n",
        "The next four layers are classification layers, the last being an output layer.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRFuSwiL9qsO",
        "outputId": "6e8acc10-2789-4337-b67b-215df0ba022f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_13 (Embedding)    (None, 500, 8)            120000    \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 4000)              0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 128)               512128    \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 642,497\n",
            "Trainable params: 642,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Embedding, Flatten\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# Embedding layers \n",
        "# network will learn 8-dimensional embeddings for each of the 15,000 words\n",
        "model.add(Embedding(15000, 8, input_length=maxlen))\n",
        "# flattens the 3D tensor output to a 2D tensor\n",
        "model.add(Flatten())\n",
        "\n",
        "# training single dense layer for classfication \n",
        "model.add(layers.Dense(128, activation = 'relu'))\n",
        "model.add(layers.Dense(64, activation = 'relu'))\n",
        "model.add(layers.Dense(32, activation = 'relu'))\n",
        "\n",
        "# output layer \n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer = 'rmsprop', \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['accuracy', f1_score])\n",
        "\n",
        "model.build()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYEkLPsoMzvu"
      },
      "source": [
        "## 3.2: Training Model 1\n",
        "\n",
        "Here I have trained the initial model using the partial training data and validated it on the validation data. I have trained for 10 epochs with a batch size of 32. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_QKdN4A6IcD",
        "outputId": "d40c479a-1f3c-4ba1-dbb3-9a2414ca5ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.5086 - accuracy: 0.7601 - f1_score: 0.2948 - val_loss: 0.4994 - val_accuracy: 0.7647 - val_f1_score: 0.3825\n",
            "Epoch 2/10\n",
            "4688/4688 [==============================] - 54s 11ms/step - loss: 0.4804 - accuracy: 0.7818 - f1_score: 0.4153 - val_loss: 0.5406 - val_accuracy: 0.7593 - val_f1_score: 0.4063\n",
            "Epoch 3/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.4441 - accuracy: 0.8078 - f1_score: 0.5089 - val_loss: 0.5623 - val_accuracy: 0.7594 - val_f1_score: 0.3720\n",
            "Epoch 4/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.4041 - accuracy: 0.8310 - f1_score: 0.5840 - val_loss: 0.6071 - val_accuracy: 0.7582 - val_f1_score: 0.2972\n",
            "Epoch 5/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.3671 - accuracy: 0.8493 - f1_score: 0.6415 - val_loss: 0.7006 - val_accuracy: 0.7376 - val_f1_score: 0.4309\n",
            "Epoch 6/10\n",
            "4688/4688 [==============================] - 60s 13ms/step - loss: 0.3338 - accuracy: 0.8657 - f1_score: 0.6887 - val_loss: 0.8624 - val_accuracy: 0.7040 - val_f1_score: 0.4539\n",
            "Epoch 7/10\n",
            "4688/4688 [==============================] - 54s 11ms/step - loss: 0.3065 - accuracy: 0.8779 - f1_score: 0.7226 - val_loss: 1.1566 - val_accuracy: 0.7398 - val_f1_score: 0.3901\n",
            "Epoch 8/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.2806 - accuracy: 0.8913 - f1_score: 0.7586 - val_loss: 1.4026 - val_accuracy: 0.7100 - val_f1_score: 0.4431\n",
            "Epoch 9/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.2587 - accuracy: 0.9021 - f1_score: 0.7828 - val_loss: 1.1227 - val_accuracy: 0.7322 - val_f1_score: 0.3805\n",
            "Epoch 10/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.2362 - accuracy: 0.9133 - f1_score: 0.8096 - val_loss: 1.8760 - val_accuracy: 0.7358 - val_f1_score: 0.3643\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_partial_train, y_partial_train,\n",
        "                    epochs = 10,\n",
        "                    batch_size = 32,\n",
        "                    validation_data = (x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ekM2g84lU8M"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boafk5Mm0Si2"
      },
      "source": [
        "# Part 4: Tuning & Regualizing\n",
        "Here I am tuning the learning rate and adding regularization to achieve the highest possible accuracy.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPNik7MV3j1l"
      },
      "source": [
        "## 4.1: Learning Rate\n",
        "\n",
        "I am tuning the learning rate of the optimiser. The learning rate determines how much the weights of the network are updated according to the accuracy achieved. If the learning rate is too small, the model may never find the optimal set of weights as they will be updated in very small amounts. If it is too large, the optimal set of weights could get skipped over during training. \n",
        "\n",
        "As I will be building many iterations of this model, I have made a `build_model` and `train_model` function to make this process more efficient, shown below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_OrpSff4Dxt"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "def build_model(learning_rate):\n",
        "  model = models.Sequential()\n",
        "  \n",
        "  # Embedding layers\n",
        "  model.add(Embedding(15000, 8, input_length=maxlen))\n",
        "  # flattens the 3D tensor output to a 2D tensor\n",
        "  model.add(Flatten())\n",
        "  \n",
        "  # training single dense layer for classfication \n",
        "  model.add(layers.Dense(128, activation = 'relu'))\n",
        "  model.add(layers.Dense(64, activation = 'relu'))\n",
        "  model.add(layers.Dense(32, activation = 'relu'))  \n",
        "  \n",
        "  # outup layer \n",
        "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "  \n",
        "  model.compile(optimizer=optimizers.RMSprop(learning_rate=learning_rate),\n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['accuracy', f1_score])\n",
        "  \n",
        "  return model\n",
        "  \n",
        "def train_model(learning_rate): \n",
        "  model = build_model(learning_rate)\n",
        "  history = model.fit(x_partial_train, y_partial_train,\n",
        "                    epochs = 10,\n",
        "                    batch_size = 32,\n",
        "                    validation_data = (x_val, y_val))\n",
        "  \n",
        "  return history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek9ULap1_F2G"
      },
      "source": [
        "### Model 2: Learning Rate 0.005\n",
        "\n",
        "In the initial model, I had the learning rate set to the default. In model 1, I am increasing it to 0.005 to see if this will help achieve a higher accuracy on the validation set. As shown below, this model starts degrading straight away, showing that this learning rate is too high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXzK-H_2472t",
        "outputId": "bc160023-49d2-42df-bf20-261c34cc037b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4688/4688 [==============================] - 69s 15ms/step - loss: 0.5171 - accuracy: 0.7565 - f1_score: 0.2520 - val_loss: 0.5065 - val_accuracy: 0.7555 - val_f1_score: 0.3966\n",
            "Epoch 2/10\n",
            "4688/4688 [==============================] - 69s 15ms/step - loss: 0.5029 - accuracy: 0.7710 - f1_score: 0.3859 - val_loss: 0.5681 - val_accuracy: 0.7506 - val_f1_score: 0.4332\n",
            "Epoch 3/10\n",
            "4688/4688 [==============================] - 68s 15ms/step - loss: 0.4851 - accuracy: 0.7843 - f1_score: 0.4191 - val_loss: 0.5586 - val_accuracy: 0.7607 - val_f1_score: 0.3811\n",
            "Epoch 4/10\n",
            "4688/4688 [==============================] - 67s 14ms/step - loss: 0.4621 - accuracy: 0.8029 - f1_score: 0.4765 - val_loss: 0.5999 - val_accuracy: 0.7605 - val_f1_score: 0.3609\n",
            "Epoch 5/10\n",
            "4688/4688 [==============================] - 68s 14ms/step - loss: 0.4302 - accuracy: 0.8208 - f1_score: 0.5387 - val_loss: 1.2113 - val_accuracy: 0.6669 - val_f1_score: 0.4992\n",
            "Epoch 6/10\n",
            "4688/4688 [==============================] - 68s 15ms/step - loss: 0.4006 - accuracy: 0.8396 - f1_score: 0.5996 - val_loss: 0.6871 - val_accuracy: 0.7166 - val_f1_score: 0.4627\n",
            "Epoch 7/10\n",
            "4688/4688 [==============================] - 68s 15ms/step - loss: 0.3712 - accuracy: 0.8531 - f1_score: 0.6359 - val_loss: 0.7015 - val_accuracy: 0.7391 - val_f1_score: 0.4056\n",
            "Epoch 8/10\n",
            "4688/4688 [==============================] - 69s 15ms/step - loss: 0.3474 - accuracy: 0.8650 - f1_score: 0.6709 - val_loss: 0.8117 - val_accuracy: 0.7489 - val_f1_score: 0.3548\n",
            "Epoch 9/10\n",
            "4688/4688 [==============================] - 67s 14ms/step - loss: 0.3258 - accuracy: 0.8754 - f1_score: 0.7026 - val_loss: 1.3049 - val_accuracy: 0.7098 - val_f1_score: 0.4431\n",
            "Epoch 10/10\n",
            "4688/4688 [==============================] - 66s 14ms/step - loss: 0.3087 - accuracy: 0.8834 - f1_score: 0.7233 - val_loss: 0.9152 - val_accuracy: 0.7399 - val_f1_score: 0.3718\n"
          ]
        }
      ],
      "source": [
        "model_2 = train_model(0.005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyR5rtvu_qnJ"
      },
      "source": [
        "### Model 3: Learning Rate 0.003\n",
        "\n",
        "In this next model, I am decreasing the learning rate to 0.003. Again, the validation accuracy starts degrading straight away, showing this learning rate is also too high for this model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbwDqQ_z_vkp",
        "outputId": "0c65a9c4-0353-41d1-d4df-ec84f31eed8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4688/4688 [==============================] - 67s 14ms/step - loss: 0.5108 - accuracy: 0.7593 - f1_score: 0.2966 - val_loss: 0.5122 - val_accuracy: 0.7629 - val_f1_score: 0.4028\n",
            "Epoch 2/10\n",
            "4688/4688 [==============================] - 68s 14ms/step - loss: 0.4766 - accuracy: 0.7832 - f1_score: 0.4347 - val_loss: 0.5046 - val_accuracy: 0.7617 - val_f1_score: 0.3455\n",
            "Epoch 3/10\n",
            "4688/4688 [==============================] - 68s 14ms/step - loss: 0.4302 - accuracy: 0.8139 - f1_score: 0.5472 - val_loss: 0.6306 - val_accuracy: 0.7368 - val_f1_score: 0.4465\n",
            "Epoch 4/10\n",
            "4688/4688 [==============================] - 68s 15ms/step - loss: 0.3810 - accuracy: 0.8425 - f1_score: 0.6305 - val_loss: 1.3153 - val_accuracy: 0.6559 - val_f1_score: 0.4744\n",
            "Epoch 5/10\n",
            "4688/4688 [==============================] - 75s 16ms/step - loss: 0.3389 - accuracy: 0.8617 - f1_score: 0.6801 - val_loss: 0.8262 - val_accuracy: 0.7521 - val_f1_score: 0.3262\n",
            "Epoch 6/10\n",
            "4688/4688 [==============================] - 68s 15ms/step - loss: 0.3055 - accuracy: 0.8809 - f1_score: 0.7305 - val_loss: 0.7888 - val_accuracy: 0.7257 - val_f1_score: 0.4176\n",
            "Epoch 7/10\n",
            "4688/4688 [==============================] - 66s 14ms/step - loss: 0.2771 - accuracy: 0.8936 - f1_score: 0.7599 - val_loss: 0.9582 - val_accuracy: 0.7323 - val_f1_score: 0.3968\n",
            "Epoch 8/10\n",
            "4688/4688 [==============================] - 65s 14ms/step - loss: 0.2509 - accuracy: 0.9055 - f1_score: 0.7893 - val_loss: 1.3750 - val_accuracy: 0.7168 - val_f1_score: 0.4158\n",
            "Epoch 9/10\n",
            "4688/4688 [==============================] - 66s 14ms/step - loss: 0.2270 - accuracy: 0.9151 - f1_score: 0.8111 - val_loss: 1.7184 - val_accuracy: 0.7069 - val_f1_score: 0.4186\n",
            "Epoch 10/10\n",
            "4688/4688 [==============================] - 65s 14ms/step - loss: 0.2083 - accuracy: 0.9253 - f1_score: 0.8365 - val_loss: 1.5289 - val_accuracy: 0.7135 - val_f1_score: 0.3895\n"
          ]
        }
      ],
      "source": [
        "model_2 = train_model(0.003)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ljEDBtVQP7"
      },
      "source": [
        "### Model 4: Learning Rate 0.0005\n",
        "\n",
        "In the next iteration I am decreasing the learning rate to 0.005 to see if this will improve on the validation accuracy compared to the initial model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEmTZmWaVGUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3deefb09-0471-447c-c09a-88eebddce964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4688/4688 [==============================] - 67s 14ms/step - loss: 0.5061 - accuracy: 0.7607 - f1_score: 0.2911 - val_loss: 0.5091 - val_accuracy: 0.7583 - val_f1_score: 0.4252\n",
            "Epoch 2/10\n",
            "4688/4688 [==============================] - 67s 14ms/step - loss: 0.4725 - accuracy: 0.7840 - f1_score: 0.4221 - val_loss: 0.5019 - val_accuracy: 0.7617 - val_f1_score: 0.2560\n",
            "Epoch 3/10\n",
            "4688/4688 [==============================] - 67s 14ms/step - loss: 0.4359 - accuracy: 0.8088 - f1_score: 0.5123 - val_loss: 0.5634 - val_accuracy: 0.7534 - val_f1_score: 0.4256\n",
            "Epoch 4/10\n",
            "4688/4688 [==============================] - 69s 15ms/step - loss: 0.3969 - accuracy: 0.8313 - f1_score: 0.5810 - val_loss: 0.6434 - val_accuracy: 0.7507 - val_f1_score: 0.3924\n",
            "Epoch 5/10\n",
            "4688/4688 [==============================] - 71s 15ms/step - loss: 0.3615 - accuracy: 0.8504 - f1_score: 0.6369 - val_loss: 0.8110 - val_accuracy: 0.7558 - val_f1_score: 0.3259\n",
            "Epoch 6/10\n",
            "4688/4688 [==============================] - 69s 15ms/step - loss: 0.3309 - accuracy: 0.8657 - f1_score: 0.6808 - val_loss: 1.2233 - val_accuracy: 0.7359 - val_f1_score: 0.4198\n",
            "Epoch 7/10\n",
            "4688/4688 [==============================] - 70s 15ms/step - loss: 0.3064 - accuracy: 0.8779 - f1_score: 0.7137 - val_loss: 1.1437 - val_accuracy: 0.7442 - val_f1_score: 0.3693\n",
            "Epoch 8/10\n",
            "4688/4688 [==============================] - 69s 15ms/step - loss: 0.2825 - accuracy: 0.8889 - f1_score: 0.7447 - val_loss: 1.0576 - val_accuracy: 0.7499 - val_f1_score: 0.3091\n",
            "Epoch 9/10\n",
            "4688/4688 [==============================] - 68s 14ms/step - loss: 0.2604 - accuracy: 0.9002 - f1_score: 0.7734 - val_loss: 1.0308 - val_accuracy: 0.7415 - val_f1_score: 0.3576\n",
            "Epoch 10/10\n",
            "4688/4688 [==============================] - 69s 15ms/step - loss: 0.2395 - accuracy: 0.9099 - f1_score: 0.7971 - val_loss: 1.2258 - val_accuracy: 0.7382 - val_f1_score: 0.3512\n"
          ]
        }
      ],
      "source": [
        "model_4 = train_model(0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbWNnIK432Us"
      },
      "source": [
        "## 4.2: Weight Regularization\n",
        "\n",
        "I am adding weight regularization, where the network is forced to set weights to small values, making the distribution of them more regular. This is done by adding a cost to the loss function for larger weights. There are two types of costs:\n",
        "\n",
        "1. L1 regularization: This is where the cost added to the weight is equivalent to the absolute value of the weight coefficients.\n",
        "2. L2 regularization: This is where the cost added to the weight is equivalent to the square of the weight coefficients.\n",
        "\n",
        "These two types of costs can be added to a network individually or simultaneously. I will be testing all three methods to find the one that works best for this model. In order to implement regularizers I have imported the regularizers module from Keras, shown below. I have also made a new build function to train the models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKmd_84urM0V"
      },
      "outputs": [],
      "source": [
        "from keras import regularizers\n",
        "\n",
        "def train_reg_model(model): \n",
        "  history = model.fit(x_partial_train, y_partial_train,\n",
        "                    epochs = 10,\n",
        "                    batch_size = 32,\n",
        "                    validation_data = (x_val, y_val))\n",
        "  \n",
        "  return history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fRvfkc6qxzS"
      },
      "source": [
        "### Model 5: L1 Regularization \n",
        "\n",
        "The first model I am training includes L1 regularization. This model peaks at epoch 5, to 0.762, which is not higher than the initial model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXpAp0dPqwC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e614499-b37b-495c-d004-fd0206d5af6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4688/4688 [==============================] - 79s 17ms/step - loss: 0.8421 - accuracy: 0.7511 - f1_score: 0.1532 - val_loss: 0.8068 - val_accuracy: 0.7571 - val_f1_score: 0.2605\n",
            "Epoch 2/10\n",
            "4688/4688 [==============================] - 77s 16ms/step - loss: 0.7980 - accuracy: 0.7621 - f1_score: 0.3044 - val_loss: 0.8068 - val_accuracy: 0.7583 - val_f1_score: 0.3582\n",
            "Epoch 3/10\n",
            "4688/4688 [==============================] - 82s 18ms/step - loss: 0.7927 - accuracy: 0.7642 - f1_score: 0.3268 - val_loss: 0.8050 - val_accuracy: 0.7593 - val_f1_score: 0.2793\n",
            "Epoch 4/10\n",
            "4688/4688 [==============================] - 82s 18ms/step - loss: 0.7892 - accuracy: 0.7653 - f1_score: 0.3430 - val_loss: 0.7946 - val_accuracy: 0.7615 - val_f1_score: 0.3175\n",
            "Epoch 5/10\n",
            "4688/4688 [==============================] - 77s 17ms/step - loss: 0.7874 - accuracy: 0.7662 - f1_score: 0.3498 - val_loss: 0.7954 - val_accuracy: 0.7617 - val_f1_score: 0.2948\n",
            "Epoch 6/10\n",
            "4688/4688 [==============================] - 83s 18ms/step - loss: 0.7857 - accuracy: 0.7672 - f1_score: 0.3585 - val_loss: 0.8111 - val_accuracy: 0.7502 - val_f1_score: 0.4451\n",
            "Epoch 7/10\n",
            "4688/4688 [==============================] - 78s 17ms/step - loss: 0.7845 - accuracy: 0.7683 - f1_score: 0.3640 - val_loss: 0.7996 - val_accuracy: 0.7604 - val_f1_score: 0.2572\n",
            "Epoch 8/10\n",
            "4688/4688 [==============================] - 79s 17ms/step - loss: 0.7833 - accuracy: 0.7696 - f1_score: 0.3696 - val_loss: 0.8124 - val_accuracy: 0.7597 - val_f1_score: 0.2508\n",
            "Epoch 9/10\n",
            "4688/4688 [==============================] - 77s 16ms/step - loss: 0.7824 - accuracy: 0.7702 - f1_score: 0.3744 - val_loss: 0.8118 - val_accuracy: 0.7545 - val_f1_score: 0.4279\n",
            "Epoch 10/10\n",
            "4688/4688 [==============================] - 77s 17ms/step - loss: 0.7817 - accuracy: 0.7710 - f1_score: 0.3791 - val_loss: 0.8281 - val_accuracy: 0.7326 - val_f1_score: 0.4797\n"
          ]
        }
      ],
      "source": [
        "model = models.Sequential()\n",
        "model.add(Embedding(15000, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(layers.Dense(128, kernel_regularizer=regularizers.l1(0.001), activation = 'relu'))\n",
        "model.add(layers.Dense(64, kernel_regularizer=regularizers.l1(0.001), activation = 'relu'))\n",
        "model.add(layers.Dense(32, kernel_regularizer=regularizers.l1(0.001),activation = 'relu'))\n",
        "\n",
        "\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer = 'rmsprop', \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['accuracy', f1_score])\n",
        "\n",
        "model.build()\n",
        "\n",
        "model_4 = train_reg_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp0v8drFsEX9"
      },
      "source": [
        "### Model 6: L2 Regulaization \n",
        "\n",
        "The second model I am training implements L2 regularization. This model peaks at epoch 3, to 0.766."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NySmRL-r8Re",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea64d202-c883-4963-8921-7c775f8fa101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4688/4688 [==============================] - 80s 17ms/step - loss: 0.5272 - accuracy: 0.7585 - f1_score: 0.2761 - val_loss: 0.5149 - val_accuracy: 0.7596 - val_f1_score: 0.2942\n",
            "Epoch 2/10\n",
            "4688/4688 [==============================] - 78s 17ms/step - loss: 0.5080 - accuracy: 0.7667 - f1_score: 0.3469 - val_loss: 0.5151 - val_accuracy: 0.7616 - val_f1_score: 0.3443\n",
            "Epoch 3/10\n",
            "4688/4688 [==============================] - 78s 17ms/step - loss: 0.4991 - accuracy: 0.7697 - f1_score: 0.3694 - val_loss: 0.5114 - val_accuracy: 0.7624 - val_f1_score: 0.3098\n",
            "Epoch 4/10\n",
            "4688/4688 [==============================] - 84s 18ms/step - loss: 0.4911 - accuracy: 0.7760 - f1_score: 0.3994 - val_loss: 0.5172 - val_accuracy: 0.7598 - val_f1_score: 0.4066\n",
            "Epoch 5/10\n",
            "4688/4688 [==============================] - 78s 17ms/step - loss: 0.4827 - accuracy: 0.7829 - f1_score: 0.4331 - val_loss: 0.5204 - val_accuracy: 0.7606 - val_f1_score: 0.3652\n",
            "Epoch 6/10\n",
            "4688/4688 [==============================] - 78s 17ms/step - loss: 0.4710 - accuracy: 0.7941 - f1_score: 0.4759 - val_loss: 0.5612 - val_accuracy: 0.7251 - val_f1_score: 0.4685\n",
            "Epoch 7/10\n",
            "4688/4688 [==============================] - 84s 18ms/step - loss: 0.4575 - accuracy: 0.8026 - f1_score: 0.5113 - val_loss: 0.5509 - val_accuracy: 0.7554 - val_f1_score: 0.3410\n",
            "Epoch 8/10\n",
            "4688/4688 [==============================] - 84s 18ms/step - loss: 0.4433 - accuracy: 0.8136 - f1_score: 0.5475 - val_loss: 0.5703 - val_accuracy: 0.7468 - val_f1_score: 0.3935\n",
            "Epoch 9/10\n",
            "4688/4688 [==============================] - 82s 18ms/step - loss: 0.4310 - accuracy: 0.8223 - f1_score: 0.5758 - val_loss: 0.5823 - val_accuracy: 0.7425 - val_f1_score: 0.3762\n",
            "Epoch 10/10\n",
            "4688/4688 [==============================] - 78s 17ms/step - loss: 0.4186 - accuracy: 0.8303 - f1_score: 0.6005 - val_loss: 0.6118 - val_accuracy: 0.7238 - val_f1_score: 0.4271\n"
          ]
        }
      ],
      "source": [
        "model = models.Sequential()\n",
        "model.add(Embedding(15000, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(layers.Dense(128, kernel_regularizer=regularizers.l2(0.001), activation = 'relu'))\n",
        "model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation = 'relu'))\n",
        "model.add(layers.Dense(32, kernel_regularizer=regularizers.l2(0.001),activation = 'relu'))\n",
        "\n",
        "\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer = 'rmsprop', \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['accuracy', f1_score])\n",
        "\n",
        "model.build()\n",
        "\n",
        "model_5 = train_reg_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpTLlZZcsJG0"
      },
      "source": [
        "### Model 7: L1 & L2 Regulaization \n",
        "\n",
        "The last model I am implementing includes L1 and L2 regularization simultaneously. This model achieved a highest validation accuracy of 0.765 at epoch 8. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDoB1ZqJsPDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc5aa2b-a382-46a1-d884-a33e41add858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4688/4688 [==============================] - 92s 19ms/step - loss: 0.8503 - accuracy: 0.7492 - f1_score: 0.1321 - val_loss: 0.8090 - val_accuracy: 0.7576 - val_f1_score: 0.3208\n",
            "Epoch 2/10\n",
            "4688/4688 [==============================] - 90s 19ms/step - loss: 0.7976 - accuracy: 0.7618 - f1_score: 0.2982 - val_loss: 0.8022 - val_accuracy: 0.7570 - val_f1_score: 0.3567\n",
            "Epoch 3/10\n",
            "4688/4688 [==============================] - 91s 20ms/step - loss: 0.7923 - accuracy: 0.7643 - f1_score: 0.3286 - val_loss: 0.8045 - val_accuracy: 0.7583 - val_f1_score: 0.2241\n",
            "Epoch 4/10\n",
            "4688/4688 [==============================] - 90s 19ms/step - loss: 0.7899 - accuracy: 0.7651 - f1_score: 0.3416 - val_loss: 0.7969 - val_accuracy: 0.7615 - val_f1_score: 0.3508\n",
            "Epoch 5/10\n",
            "4688/4688 [==============================] - 92s 20ms/step - loss: 0.7880 - accuracy: 0.7659 - f1_score: 0.3465 - val_loss: 0.8198 - val_accuracy: 0.7440 - val_f1_score: 0.4511\n",
            "Epoch 6/10\n",
            "4688/4688 [==============================] - 87s 19ms/step - loss: 0.7871 - accuracy: 0.7667 - f1_score: 0.3522 - val_loss: 0.8110 - val_accuracy: 0.7462 - val_f1_score: 0.4440\n",
            "Epoch 7/10\n",
            "4688/4688 [==============================] - 73s 16ms/step - loss: 0.7864 - accuracy: 0.7672 - f1_score: 0.3567 - val_loss: 0.8072 - val_accuracy: 0.7588 - val_f1_score: 0.2373\n",
            "Epoch 8/10\n",
            "4688/4688 [==============================] - 73s 16ms/step - loss: 0.7857 - accuracy: 0.7688 - f1_score: 0.3657 - val_loss: 0.7973 - val_accuracy: 0.7613 - val_f1_score: 0.3710\n",
            "Epoch 9/10\n",
            "4688/4688 [==============================] - 73s 16ms/step - loss: 0.7849 - accuracy: 0.7688 - f1_score: 0.3685 - val_loss: 0.8045 - val_accuracy: 0.7547 - val_f1_score: 0.3815\n",
            "Epoch 10/10\n",
            "4688/4688 [==============================] - 73s 16ms/step - loss: 0.7843 - accuracy: 0.7691 - f1_score: 0.3684 - val_loss: 0.8000 - val_accuracy: 0.7554 - val_f1_score: 0.4123\n"
          ]
        }
      ],
      "source": [
        "model = models.Sequential()\n",
        "model.add(Embedding(15000, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(layers.Dense(128, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001), activation = 'relu'))\n",
        "model.add(layers.Dense(64, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001), activation = 'relu'))\n",
        "model.add(layers.Dense(32, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),activation = 'relu'))\n",
        "\n",
        "\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer = 'rmsprop', \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['accuracy', f1_score])\n",
        "\n",
        "model.build()\n",
        "\n",
        "model_6 = train_reg_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGnuOWwP38pE"
      },
      "source": [
        "## 4.4: Dropout \n",
        "\n",
        "Dropout is another regularizing technique which I will be implementing. It drops, i.e., sets to zero, a number of output features of a model during training, breaking apart coincidental patterns that aren’t significant, allowing the final model to be more generalisable. I have remade the build_model and train_model functions, shown below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tel9KsK-9xkd"
      },
      "outputs": [],
      "source": [
        "def build_drop_model(dropout):\n",
        "  model = models.Sequential()\n",
        "  model.add(Embedding(15000, 8, input_length=maxlen))\n",
        "  model.add(Flatten())\n",
        "  model.add(layers.Dense(128, activation = 'relu'))\n",
        "  model.add(layers.Dropout(dropout)) # dropout layer\n",
        "  model.add(layers.Dense(64, activation = 'relu'))\n",
        "  model.add(layers.Dropout(dropout)) # dropout layer\n",
        "  model.add(layers.Dense(32, kernel_regularizer=regularizers.l2(0.001),activation = 'relu'))\n",
        "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "  \n",
        "  model.compile(optimizer = 'rmsprop', \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['accuracy', f1_score])\n",
        "  \n",
        "  return model\n",
        "\n",
        "def train_drop_model(dropout): \n",
        "  model = build_drop_model(dropout)\n",
        "  history = model.fit(x_partial_train, y_partial_train,\n",
        "                    epochs = 10,\n",
        "                    batch_size = 32,\n",
        "                    validation_data = (x_val, y_val))\n",
        "  \n",
        "  return history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAE5tl5--ZAq"
      },
      "source": [
        "### Model 8: Dropout 30%\n",
        "I am stating by adding a dropout rate of 30% to two layers in the model. This has not significantly changed the highest validation accuracy, which is still at 0.765. This could be because the dropout rate was either too high or too low. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8V2wKRcGA4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31abe4e-ae7f-45e7-8dca-4aa34ecb3ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4688/4688 [==============================] - 57s 12ms/step - loss: 0.5214 - accuracy: 0.7582 - f1_score: 0.2798 - val_loss: 0.5044 - val_accuracy: 0.7670 - val_f1_score: 0.3514\n",
            "Epoch 2/10\n",
            "4688/4688 [==============================] - 57s 12ms/step - loss: 0.4972 - accuracy: 0.7730 - f1_score: 0.3741 - val_loss: 0.5244 - val_accuracy: 0.7684 - val_f1_score: 0.3595\n",
            "Epoch 3/10\n",
            "4688/4688 [==============================] - 57s 12ms/step - loss: 0.4765 - accuracy: 0.7875 - f1_score: 0.4425 - val_loss: 0.5040 - val_accuracy: 0.7677 - val_f1_score: 0.3548\n",
            "Epoch 4/10\n",
            "4688/4688 [==============================] - 55s 12ms/step - loss: 0.4476 - accuracy: 0.8057 - f1_score: 0.5075 - val_loss: 0.5224 - val_accuracy: 0.7579 - val_f1_score: 0.4393\n",
            "Epoch 5/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.4206 - accuracy: 0.8224 - f1_score: 0.5622 - val_loss: 0.5222 - val_accuracy: 0.7628 - val_f1_score: 0.3680\n",
            "Epoch 6/10\n",
            "4688/4688 [==============================] - 58s 12ms/step - loss: 0.3962 - accuracy: 0.8353 - f1_score: 0.6019 - val_loss: 0.5657 - val_accuracy: 0.7535 - val_f1_score: 0.4360\n",
            "Epoch 7/10\n",
            "4688/4688 [==============================] - 58s 12ms/step - loss: 0.3761 - accuracy: 0.8467 - f1_score: 0.6346 - val_loss: 0.5675 - val_accuracy: 0.7537 - val_f1_score: 0.3965\n",
            "Epoch 8/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.3569 - accuracy: 0.8581 - f1_score: 0.6645 - val_loss: 0.6079 - val_accuracy: 0.7576 - val_f1_score: 0.3579\n",
            "Epoch 9/10\n",
            "4688/4688 [==============================] - 55s 12ms/step - loss: 0.3415 - accuracy: 0.8661 - f1_score: 0.6877 - val_loss: 0.7223 - val_accuracy: 0.7440 - val_f1_score: 0.4305\n",
            "Epoch 10/10\n",
            "4688/4688 [==============================] - 57s 12ms/step - loss: 0.3257 - accuracy: 0.8745 - f1_score: 0.7094 - val_loss: 0.6670 - val_accuracy: 0.7527 - val_f1_score: 0.3940\n"
          ]
        }
      ],
      "source": [
        "model_7 = train_drop_model(0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoaZ2_lWGYE-"
      },
      "source": [
        "### Model 9: Dropout 20%\n",
        "\n",
        "I have dropped the dropout rate to 20% for the next model. This has increased the highest validation accuracy to 0.767 at epoch 2. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyEeQqslGXbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00e5b9d-2bab-4059-bf6d-517f148addfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.5173 - accuracy: 0.7595 - f1_score: 0.2875 - val_loss: 0.5091 - val_accuracy: 0.7654 - val_f1_score: 0.3619\n",
            "Epoch 2/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.4920 - accuracy: 0.7752 - f1_score: 0.3926 - val_loss: 0.5139 - val_accuracy: 0.7667 - val_f1_score: 0.3315\n",
            "Epoch 3/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.4686 - accuracy: 0.7921 - f1_score: 0.4676 - val_loss: 0.5062 - val_accuracy: 0.7662 - val_f1_score: 0.3996\n",
            "Epoch 4/10\n",
            "4688/4688 [==============================] - 55s 12ms/step - loss: 0.4346 - accuracy: 0.8129 - f1_score: 0.5357 - val_loss: 0.5330 - val_accuracy: 0.7645 - val_f1_score: 0.3368\n",
            "Epoch 5/10\n",
            "4688/4688 [==============================] - 57s 12ms/step - loss: 0.4017 - accuracy: 0.8318 - f1_score: 0.5900 - val_loss: 0.5875 - val_accuracy: 0.7459 - val_f1_score: 0.4550\n",
            "Epoch 6/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.3749 - accuracy: 0.8458 - f1_score: 0.6289 - val_loss: 0.6007 - val_accuracy: 0.7589 - val_f1_score: 0.3239\n",
            "Epoch 7/10\n",
            "4688/4688 [==============================] - 55s 12ms/step - loss: 0.3511 - accuracy: 0.8582 - f1_score: 0.6645 - val_loss: 0.6584 - val_accuracy: 0.7528 - val_f1_score: 0.3949\n",
            "Epoch 8/10\n",
            "4688/4688 [==============================] - 50s 11ms/step - loss: 0.3316 - accuracy: 0.8684 - f1_score: 0.6906 - val_loss: 0.6546 - val_accuracy: 0.7479 - val_f1_score: 0.4076\n",
            "Epoch 9/10\n",
            "4688/4688 [==============================] - 52s 11ms/step - loss: 0.3132 - accuracy: 0.8769 - f1_score: 0.7151 - val_loss: 0.7861 - val_accuracy: 0.7197 - val_f1_score: 0.4614\n",
            "Epoch 10/10\n",
            "4688/4688 [==============================] - 53s 11ms/step - loss: 0.2987 - accuracy: 0.8844 - f1_score: 0.7333 - val_loss: 0.7023 - val_accuracy: 0.7433 - val_f1_score: 0.3974\n"
          ]
        }
      ],
      "source": [
        "model_8 = train_drop_model(0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-ZyaDSpVHGp"
      },
      "source": [
        "### Model 10: Dropout 10%\n",
        "\n",
        "I am now decreasing the dropout to 10%, hoping it caused validation accuracy to rise even more compared to model 8. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzTSUmhEVDoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed2a461-f621-4681-99d2-b99e4bedcb90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4688/4688 [==============================] - 57s 12ms/step - loss: 0.5126 - accuracy: 0.7612 - f1_score: 0.2972 - val_loss: 0.5080 - val_accuracy: 0.7666 - val_f1_score: 0.3703\n",
            "Epoch 2/10\n",
            "4688/4688 [==============================] - 55s 12ms/step - loss: 0.4870 - accuracy: 0.7771 - f1_score: 0.3994 - val_loss: 0.4992 - val_accuracy: 0.7658 - val_f1_score: 0.4158\n",
            "Epoch 3/10\n",
            "4688/4688 [==============================] - 54s 12ms/step - loss: 0.4614 - accuracy: 0.7952 - f1_score: 0.4797 - val_loss: 0.5176 - val_accuracy: 0.7600 - val_f1_score: 0.4457\n",
            "Epoch 4/10\n",
            "4688/4688 [==============================] - 56s 12ms/step - loss: 0.4246 - accuracy: 0.8185 - f1_score: 0.5537 - val_loss: 0.5277 - val_accuracy: 0.7564 - val_f1_score: 0.4450\n",
            "Epoch 5/10\n",
            "4688/4688 [==============================] - 57s 12ms/step - loss: 0.3878 - accuracy: 0.8379 - f1_score: 0.6107 - val_loss: 0.5564 - val_accuracy: 0.7581 - val_f1_score: 0.3823\n",
            "Epoch 6/10\n",
            "4688/4688 [==============================] - 58s 12ms/step - loss: 0.3569 - accuracy: 0.8555 - f1_score: 0.6532 - val_loss: 0.6503 - val_accuracy: 0.7499 - val_f1_score: 0.4179\n",
            "Epoch 7/10\n",
            "4688/4688 [==============================] - 55s 12ms/step - loss: 0.3295 - accuracy: 0.8683 - f1_score: 0.6898 - val_loss: 0.7043 - val_accuracy: 0.7376 - val_f1_score: 0.4440\n",
            "Epoch 8/10\n",
            "4688/4688 [==============================] - 57s 12ms/step - loss: 0.3072 - accuracy: 0.8791 - f1_score: 0.7171 - val_loss: 0.7073 - val_accuracy: 0.7382 - val_f1_score: 0.4214\n",
            "Epoch 9/10\n",
            "4688/4688 [==============================] - 57s 12ms/step - loss: 0.2857 - accuracy: 0.8903 - f1_score: 0.7492 - val_loss: 0.7856 - val_accuracy: 0.7429 - val_f1_score: 0.4015\n",
            "Epoch 10/10\n",
            "4688/4688 [==============================] - 57s 12ms/step - loss: 0.2668 - accuracy: 0.8995 - f1_score: 0.7719 - val_loss: 0.8744 - val_accuracy: 0.7289 - val_f1_score: 0.4325\n"
          ]
        }
      ],
      "source": [
        "model_8 = train_drop_model(0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOocH2y2dczx"
      },
      "source": [
        "## 5.1: Training Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asIbIICb6hxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623f25e9-fc3c-4cac-e1ed-9164e2108348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "6250/6250 [==============================] - 67s 11ms/step - loss: 0.5092 - accuracy: 0.7618 - f1_score: 0.3102\n",
            "Epoch 2/2\n",
            "6250/6250 [==============================] - 71s 11ms/step - loss: 0.4925 - accuracy: 0.7747 - f1_score: 0.3869\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84c689a0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Embedding, Flatten\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(Embedding(15000, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(layers.Dense(128,  activation = 'relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(64,  activation = 'relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(32, activation = 'relu'))\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer = 'rmsprop', \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['accuracy', f1_score])\n",
        "\n",
        "model.fit(x_train, y_train, epochs = 2, batch_size = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6MRCO4NEtx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76da76fb-2339-4574-a65d-2ac2cb138bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3125/3125 [==============================] - 9s 3ms/step - loss: 0.5217 - accuracy: 0.7571 - f1_score: 0.4644\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moiYzYIwQpBU"
      },
      "outputs": [],
      "source": [
        "y_pred = (model.predict(x_test) > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvWout5CRNix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab32f14-d55d-4ae5-c882-8e8f938e9f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.76\n",
            "Precision score: 0.55\n",
            "Recall score: 0.43\n",
            "F1 score: 0.48\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('Accuracy: %.2f' % (accuracy_score(y_test, y_pred)))\n",
        "print('Precision score: %.2f' % (precision_score(y_test, y_pred)))\n",
        "print('Recall score: %.2f' % (recall_score(y_test, y_pred)))\n",
        "print('F1 score: %.2f' % (f1_score(y_test, y_pred)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Set1:BagOfWords-Final.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMCK17BhTSTHVgzqUEKurN/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}